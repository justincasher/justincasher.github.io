---
layout: Math
indent: true
permalink: /a-course-on-llms
feedformat: card
title: A course on LLMs
---
<br>
**Abstract.** &nbsp; We trace through the different definitions of commonly used LLM architectures. Our goal is to understand how they relate to eachother and potential points of improvement.



## Table of Contents
1. [Neural networks](#1-neural-networks)
2. [Attention](#2-attention)
3. [Recursive neural networks](#3-recursive-neural-networks)
4. [State-space models](#4-state-space-models)
5. [References](#5-references)



## 1. Neural networks

&emsp; Let $ R $ be a ring. A *neural network* $ f \colon R^{d_1} \to R^{d_n} $ is a composition of functions $ f = g_n \circ \cdots \circ g_1$ between free $ R $-modules $ g_j \colon R^{d_j} \to R^{d_{j+1}} $. We call every coordinate in $ R^{d_j} $ a *neuron*, $ R^{d_1} $ the *input layer*, $ R^{d_n} $ the *output layer*, and each $ R^{d_j} $, $ 1 < d_j < n $ a *hidden layer*. Commonly, $ R $ is taken to be the real numbers $ \mathbb{R} $ or the complex numbers $ \mathbb{C} $.

&emsp; Neural 



## 2. Attention





## 3. Recursive neural networks





## 4. State-space models





## 5. References

1. 